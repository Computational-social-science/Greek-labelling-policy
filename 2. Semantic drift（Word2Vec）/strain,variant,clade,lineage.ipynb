{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":[],"mount_file_id":"1X78O46LpZxnmIjRlHA7hARRWfKsOftxN","authorship_tag":"ABX9TyPqZ9PBlTVa4DuDUkCk00YG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Load functions and packages"],"metadata":{"id":"eWch57ylCp6m"}},{"cell_type":"code","source":["pip install xlrd==1.2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZW4bL7gGTBGo","executionInfo":{"status":"ok","timestamp":1659403390631,"user_tz":-480,"elapsed":9045,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"6fffbac6-1da6-4dd1-e13d-06850aae627c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting xlrd==1.2.0\n","  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 7.8 MB/s \n","\u001b[?25hInstalling collected packages: xlrd\n","  Attempting uninstall: xlrd\n","    Found existing installation: xlrd 1.1.0\n","    Uninstalling xlrd-1.1.0:\n","      Successfully uninstalled xlrd-1.1.0\n","Successfully installed xlrd-1.2.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import xlrd #读取excel数据\n","import re\n","import nltk\n","from nltk.tokenize import MWETokenizer\n","import re\n","import sys, time\n","from matplotlib import pyplot as plt\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from scipy.linalg import orthogonal_procrustes\n","from gensim.models import word2vec\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","from mpl_toolkits import mplot3d\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0v3QT773Psw","executionInfo":{"status":"ok","timestamp":1659403456034,"user_tz":-480,"elapsed":300,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"baa301e4-d72b-4a71-8140-f19aa9b8c2af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def stopwordslist(filepath):\n","    \"\"\"\n","        从文件导入停用词表\n","        :param filepath: 停用词文件的地址\n","        :return: 停用词列表\n","    \"\"\"\n","    stopwords=[line.strip() for line in open(filepath,'r',encoding='utf-8').readlines()]\n","    return stopwords"],"metadata":{"id":"Ufk2bMGH3SHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 获取单词的词性\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return None\n","def preprocess(doc_set,stopwords):\n","    \"\"\"\n","        处理语料（数据清洗+分词+去停用词）\n","        :param doc_set: 语料库的数据，类型为list\n","        :param stopwords: 停用词列表\n","        :return: 处理好的文章关键词\n","    \"\"\"\n","    texts = []#每篇文章关键词\n","    for doc in doc_set:\n","        doc=doc.lower()\n","        # 数据清洗，删除数字以及特殊符号\n","        cop=re.compile('\\d')\n","        cleaned_doc=cop.sub('',doc)\n","        cop=re.compile('[.:/()\\?!\\(\\)\\[\\]\\{\\}\\*\\+\\\\\\|\\\"\\<\\>,~@#$%^&\\-+=;\\'\\`–“”—_·]')\n","        cleaned_doc=cop.sub('',cleaned_doc)\n","        # 小写化\n","        cleaned_doc=cleaned_doc.lower()\n","        # 分词\n","        doc_cut=nltk.word_tokenize(cleaned_doc)\n","        # print(doc_cut)\n","        # 获取词的词性后进行词形还原\n","        tagged_sent = pos_tag(doc_cut)\n","        wnl = WordNetLemmatizer()\n","        lemmas_sent = [] # 词形还原后的结果\n","        for tag in tagged_sent:\n","          wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n","          lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n","        # 词干提取\n","        # ps = PorterStemmer()\n","        # porter = []\n","        # for w in lemmas_sent:\n","        #   porter.append(ps.stem(w))\n","        # cleaned_doc=porter_stemmer.stem(cleaned_doc)\n","        # 去停用词\n","        text_list = [word for word in lemmas_sent if word not in stopwords and len(word)>2 and len(word)<25]\n","        # print(text_list)\n","        texts.append(text_list)\n","    return texts"],"metadata":{"id":"-rz_t-_o1OC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUofihAr63Ra","executionInfo":{"status":"ok","timestamp":1659403465047,"user_tz":-480,"elapsed":335,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"e4b8e604-93d4-4426-8d11-c80edfa328a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/Colab\\ Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bU5wF5UL6_nF","executionInfo":{"status":"ok","timestamp":1659403466660,"user_tz":-480,"elapsed":2,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"99e7a5a2-66dd-4f02-d347-aa90212f1074"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["stopwords = stopwordslist(r\"stopwords_English+Latin.txt\")"],"metadata":{"id":"w07zw-XoAkjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Word2Vec"],"metadata":{"id":"w_FrwHK7Cfxi"}},{"cell_type":"markdown","source":["## 1570-1620"],"metadata":{"id":"wjUwABJlBWMQ"}},{"cell_type":"code","source":["year = \"1570-1620\"\n","doc_set = []\n","workbook = xlrd.open_workbook(\"lineage, variant, clade, strain_allData.xlsx\")\n","sheet = workbook.sheet_by_name(year)\n","for index in range(sheet.nrows):\n","  doc_set.append(sheet.row_values(index)[0])\n","texts = preprocess(doc_set, stopwords)\n","# print(texts)\n","t=''\n","j=0\n","with open (year+'.txt','w') as q:\n","    for i in texts:\n","      for e in range(len(texts[j])):\n","        t=t+str(i[e])+' '\n","      j+=1\n","      q.write(t.strip(' '))\n","      q.write('\\n')\n","      t=''\n","sentences = word2vec.Text8Corpus(year+'.txt')\n","model_1500 = word2vec.Word2Vec(sentences=sentences, size=100, window = 8, iter = 10) # CBOW算法\n","# window = 8：控制窗口，如果设得较小，那么模型学习到的是词汇间的组合性关系（词性相异）；如果设置得较大，会学习到词汇之间的聚合性关系（词性相同）。模型默认的window数值为5\n","# size = 100：size是输出词向量的维数，即神经网络的隐藏层的单元数\n","# iter = 10：假如参与训练的文本量较少，就需要把这个参数调大一些\n","model_1500.save('word2vec_1570-1620.bin')"],"metadata":{"id":"ZP6qOkbH6Wvy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1670-1720"],"metadata":{"id":"42tMl2thC6dJ"}},{"cell_type":"code","source":["year = \"1670-1720\"\n","doc_set = []\n","workbook = xlrd.open_workbook(\"lineage, variant, clade, strain_allData.xlsx\")\n","sheet = workbook.sheet_by_name(year)\n","for index in range(sheet.nrows):\n","  doc_set.append(sheet.row_values(index)[0])\n","texts = preprocess(doc_set, stopwords)\n","# print(texts)\n","t=''\n","j=0\n","with open (year+'.txt','w') as q:\n","    for i in texts:\n","      for e in range(len(texts[j])):\n","        t=t+str(i[e])+' '\n","      j+=1\n","      q.write(t.strip(' '))\n","      q.write('\\n')\n","      t=''\n","from gensim.models import word2vec\n","sentences = word2vec.Text8Corpus(year+'.txt')\n","model_1540 = word2vec.Word2Vec(sentences=sentences, size=100, window = 8, iter = 10)\n","model_1540.save('word2vec_1670-1720.bin')"],"metadata":{"id":"w9LNH_0Ef9vk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1770-1820"],"metadata":{"id":"aLcey-UXDIXd"}},{"cell_type":"code","source":["year = \"1770-1820\"\n","doc_set = []\n","workbook = xlrd.open_workbook(\"lineage, variant, clade, strain_allData.xlsx\")\n","sheet = workbook.sheet_by_name(year)\n","for index in range(sheet.nrows):\n","  doc_set.append(sheet.row_values(index)[0])\n","texts = preprocess(doc_set, stopwords)\n","# print(texts)\n","t=''\n","j=0\n","with open (year+'.txt','w') as q:\n","    for i in texts:\n","      for e in range(len(texts[j])):\n","        t=t+str(i[e])+' '\n","      j+=1\n","      q.write(t.strip(' '))\n","      q.write('\\n')\n","      t=''\n","from gensim.models import word2vec\n","sentences = word2vec.Text8Corpus(year+'.txt')\n","model_1610 = word2vec.Word2Vec(sentences=sentences, size=100, window = 8, iter = 10)\n","model_1610.save('word2vec_1770-1820.bin')"],"metadata":{"id":"n9EpsFptDKhr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1870-1920"],"metadata":{"id":"gZ-bmKWLDVeu"}},{"cell_type":"code","source":["year = \"1870-1920\"\n","doc_set = []\n","workbook = xlrd.open_workbook(\"lineage, variant, clade, strain_allData.xlsx\")\n","sheet = workbook.sheet_by_name(year)\n","for index in range(sheet.nrows):\n","  doc_set.append(sheet.row_values(index)[0])\n","texts = preprocess(doc_set, stopwords)\n","# print(texts)\n","t=''\n","j=0\n","with open (year+'.txt','w') as q:\n","    for i in texts:\n","      for e in range(len(texts[j])):\n","        t=t+str(i[e])+' '\n","      j+=1\n","      q.write(t.strip(' '))\n","      q.write('\\n')\n","      t=''\n","from gensim.models import word2vec\n","sentences = word2vec.Text8Corpus(year+'.txt')\n","model_1680 = word2vec.Word2Vec(sentences=sentences, size=100, window = 8, iter = 10)\n","model_1680.save('word2vec_1870-1920.bin')"],"metadata":{"id":"mpr4C_70DXKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1970-2020"],"metadata":{"id":"bmmXfcagDoyi"}},{"cell_type":"code","source":["year = \"1970-2020\"\n","doc_set = []\n","workbook = xlrd.open_workbook(\"lineage, variant, clade, strain_allData.xlsx\")\n","sheet = workbook.sheet_by_name(year)\n","for index in range(sheet.nrows):\n","  doc_set.append(sheet.row_values(index)[0])\n","texts = preprocess(doc_set, stopwords)\n","# print(texts)\n","t=''\n","j=0\n","with open (year+'.txt','w') as q:\n","    for i in texts:\n","      for e in range(len(texts[j])):\n","        t=t+str(i[e])+' '\n","      j+=1\n","      q.write(t.strip(' '))\n","      q.write('\\n')\n","      t=''\n","from gensim.models import word2vec\n","sentences = word2vec.Text8Corpus(year+'.txt')\n","model_1750 = word2vec.Word2Vec(sentences=sentences, size=100, window = 8, iter = 10)\n","model_1750.save('word2vec_1970-2020.bin')"],"metadata":{"id":"5JHen2YQDqN7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save model"],"metadata":{"id":"ehdwY7-mjWzn"}},{"cell_type":"code","source":["import xlwt\n","def save_file(year,words):\n","  workbook = xlwt.Workbook(encoding = 'ascii')  \n","  model = word2vec.Word2Vec.load('word2vec_'+year+'.bin')\n","  for word in words:\n","    row = 0\n","    worksheet = workbook.add_sheet(word)\n","    worksheet.write(row, 0, \"word\")\n","    worksheet.write(row, 1, \"similarity\")\n","    row+=1\n","    for i in model.most_similar(word,topn=500):      \n","      # 保存数据前相似的\n","      worksheet.write(row, 0, i[0]) # 相似词\n","      worksheet.write(row, 1, i[1]) # 相似概率\n","      row+=1\n","    workbook.save('similarity_'+year+'.xls')  # 保存文件"],"metadata":{"id":"AIIxKKkklyt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 归一化后再存入\n","import xlwt\n","def MaxMinNormalization(x,Max,Min):\n","    x = 100*(x - Min) / (Max - Min); #归一化到0-100\n","    return x;\n","def save_file(year,words):\n","  workbook = xlwt.Workbook(encoding = 'ascii')  \n","  model = word2vec.Word2Vec.load('word2vec_'+year+'.bin')\n","  for word in words:\n","    row = 0\n","    worksheet = workbook.add_sheet(word)\n","    worksheet.write(row, 0, \"word\")\n","    worksheet.write(row, 1, \"similarity\")\n","    worksheet.write(row, 2, \"normalized\")\n","    row+=1\n","    first = 0\n","    max = 0\n","    min = 0\n","    for i in model.most_similar(word,topn=200):\n","      # 保存数据前相似的\n","      worksheet.write(row, 0, i[0]) # 相似词\n","      worksheet.write(row, 1, i[1]) # 相似概率\n","      row+=1\n","      if(first==0):\n","        max=i[1]\n","        min=i[1]\n","        first=1\n","      else:\n","        if(i[1]>max):\n","          max=i[1]\n","        if(i[1]<min):\n","          min=i[1]\n","    normal_row = 1\n","    for i in model.most_similar(word,topn=50):\n","      worksheet.write(normal_row, 2\n","                , round(MaxMinNormalization(i[1],max,min),2)) # 归一化\n","      normal_row+=1\n","    workbook.save('similarity_'+year+'.xls')  # 保存文件"],"metadata":{"id":"PizRO-OgEDeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_file(\"1670-1720\",[\"variant\",\"clade\",\"strain\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MT4QpbFP1y7T","executionInfo":{"status":"ok","timestamp":1659403905256,"user_tz":-480,"elapsed":1188,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"9408a4c2-55ec-4e49-a6d7-8ce09af6a373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","source":["save_file(\"1770-1820\",[\"lineage\",\"variant\",\"clade\",\"strain\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2shAduJMXCJ","executionInfo":{"status":"ok","timestamp":1659404138716,"user_tz":-480,"elapsed":1459,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"91c16ae6-ca51-480a-9657-0e0eb14e0c81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","source":["save_file(\"1870-1920\",[\"lineage\",\"variant\",\"clade\",\"strain\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8h_Od81Mb75","executionInfo":{"status":"ok","timestamp":1659404152409,"user_tz":-480,"elapsed":1431,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"06b6e027-2e1a-45f2-f0a0-03b8fb2e5593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","source":["save_file(\"1970-2020\",[\"lineage\",\"variant\",\"clade\",\"strain\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8APkXw2D_3Rs","executionInfo":{"status":"ok","timestamp":1659404156119,"user_tz":-480,"elapsed":1680,"user":{"displayName":"陈雅","userId":"14288524600864903436"}},"outputId":"29d46d7f-c64d-4345-809e-bfdcb7057b07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"markdown","source":["# Load model"],"metadata":{"id":"Lj3my-PwcEGS"}},{"cell_type":"code","source":["model_1 = word2vec.Word2Vec.load('word2vec_1970-2020.bin')\n","for i in model_1.most_similar(\"lineage\",topn=40):\n","  print(i[0],i[1])"],"metadata":{"id":"y5HWgWDJN8lu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2 = word2vec.Word2Vec.load('word2vec_1870-1920.bin')\n","for i in model_2.most_similar(\"variation\",topn=40):\n","  print(i[0],i[1])"],"metadata":{"id":"CReJeK0AZkCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3 = word2vec.Word2Vec.load('word2vec_1770-1820.bin')\n","for i in model_3.most_similar(\"agros\",topn=100):\n","  print(i[0],i[1])"],"metadata":{"id":"uQs8e2r5aAC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_4 = word2vec.Word2Vec.load('word2vec_1670-1720.bin')\n","for i in model_4.most_similar(\"venenum\",topn=40):\n","  print(i[0],i[1])"],"metadata":{"id":"bYzgwOpraqoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_5 = word2vec.Word2Vec.load('word2vec_1570-1620.bin')\n","for i in model_5.most_similar(\"religionis\",topn=40):\n","  print(i[0],i[1])"],"metadata":{"id":"FtjZYN7cXb1G"},"execution_count":null,"outputs":[]}]}